"""
Behavioral cloning pre-training for LLLMaE-PPO.
This script trains a model using behavioral cloning on pre-collected trajectory data generated by LLM.
"""

import os
import pickle

import gymnasium as gym
import hydra
import numpy as np
import torch
import torch.nn as nn
import torch.optim as optim
from minigrid.wrappers import FlatObsWrapper
from omegaconf import DictConfig
from torch.utils.data import DataLoader, Dataset

from utils import set_seed


def layer_init(layer, std=np.sqrt(2), bias_const=0.0):
    torch.nn.init.orthogonal_(layer.weight, std)
    torch.nn.init.constant_(layer.bias, bias_const)
    return layer


class MiniGridDataset(Dataset):
    def __init__(self, state_action_pairs):
        states = [pair["state"] for pair in state_action_pairs]
        actions = [pair["action"] for pair in state_action_pairs]
        print(
            f"Size of state: {states[0].shape if isinstance(states[0], np.ndarray) else len(states[0])}"
        )
        print(f"actions: {actions[0]}")
        self.states = torch.tensor(np.array(states, dtype=np.float32))
        self.actions = torch.tensor(np.array(actions, dtype=np.int64))

    def __len__(self):
        return len(self.states)

    def __getitem__(self, idx):
        return self.states[idx], self.actions[idx]


class BehavioralCloningModel(nn.Module):
    def __init__(self, input_size, output_size, hidden_size=64):
        super(BehavioralCloningModel, self).__init__()
        self.actor = nn.Sequential(
            layer_init(nn.Linear(input_size, hidden_size)),
            nn.Tanh(),
            layer_init(nn.Linear(hidden_size, hidden_size)),
            nn.Tanh(),
            layer_init(nn.Linear(hidden_size, output_size), std=0.01),
        )

    def forward(self, state: torch.Tensor) -> torch.Tensor:
        return self.actor(state)


def load_pkl_files(pkl_dir):
    all_pairs = []
    for pkl_file in os.listdir(pkl_dir):
        if pkl_file.endswith(".pkl"):
            file_path = os.path.join(pkl_dir, pkl_file)
            with open(file_path, "rb") as f:
                state_action_pairs = pickle.load(f)
                all_pairs.extend(state_action_pairs)
            print(f"Loaded {len(state_action_pairs)} pairs from {pkl_file}")

    print(f"Total state-action pairs loaded: {len(all_pairs)}")
    return all_pairs


def get_model_dimensions(data_pairs, env_name, max_steps):
    sample_state = data_pairs[0]["state"]
    input_size = (
        sample_state.shape[0]
        if isinstance(sample_state, np.ndarray)
        else len(sample_state)
    )

    env = gym.make(env_name, max_steps=max_steps)
    env = FlatObsWrapper(env)
    output_size = env.action_space.n
    env.close()

    return input_size, output_size


def evaluate_success_rate(model, env_name, max_steps, num_episodes=10):
    model.eval()
    successes = 0

    for episode in range(num_episodes):
        env = gym.make(env_name, max_steps=max_steps)
        env = FlatObsWrapper(env)
        obs, _ = env.reset()
        done = False

        while not done:
            state = torch.tensor(
                obs.astype(np.float32).flatten(), dtype=torch.float32
            ).unsqueeze(0)
            with torch.no_grad():
                action_probs = torch.softmax(model(state), dim=1)
                action = torch.argmax(action_probs, dim=1).item()

            obs, reward, terminated, truncated, _ = env.step(action)
            done = terminated or truncated

            if terminated and reward > 0:
                successes += 1
                break

        env.close()

    return successes / num_episodes


def train_bc_model(model, data_loader, cfg, device="cpu"):
    criterion = nn.CrossEntropyLoss()
    optimizer = optim.Adam(model.parameters(), lr=cfg.model.lr)

    success_rate_threshold = cfg.train.get("success_rate_threshold", 0.15)
    eval_frequency = cfg.train.get("eval_frequency", 10)

    print(
        f"Training BC model with early stopping at {success_rate_threshold * 100:.0f}% success rate"
    )

    for epoch in range(cfg.train.num_epochs):
        model.train()
        epoch_loss = 0.0

        for states, actions in data_loader:
            states, actions = states.to(device), actions.to(device)
            optimizer.zero_grad()
            outputs = model(states)
            loss = criterion(outputs, actions)
            loss.backward()
            optimizer.step()
            epoch_loss += loss.item()

        avg_loss = epoch_loss / len(data_loader)

        if (epoch + 1) % eval_frequency == 0:
            success_rate = evaluate_success_rate(
                model, cfg.env.name, cfg.env.max_episode_steps
            )
            print(
                f"Epoch {epoch + 1}: Loss={avg_loss:.4f}, Success Rate={success_rate:.3f}"
            )

            if success_rate >= success_rate_threshold:
                print(
                    f"Early stopping: Success rate {success_rate:.3f} >= threshold {success_rate_threshold:.3f}"
                )
                break
        elif (epoch + 1) % 50 == 0:
            print(f"Epoch {epoch + 1}: Loss={avg_loss:.4f}")

    return model


@hydra.main(
    config_path="configs",
    config_name="bc",
    version_base="1.1",
)
def main(cfg: DictConfig):
    device = torch.device("cuda" if torch.cuda.is_available() and cfg.cuda else "cpu")
    print(f"Using device: {device}")

    set_seed(cfg.seed)

    all_pairs = load_pkl_files(cfg.data.training_data_dir)
    dataset = MiniGridDataset(all_pairs)
    data_loader = DataLoader(dataset, batch_size=cfg.train.batch_size, shuffle=True)

    input_size, output_size = get_model_dimensions(
        all_pairs, cfg.env.name, cfg.env.max_episode_steps
    )
    print(f"Model: input_size={input_size}, output_size={output_size}")

    model = BehavioralCloningModel(
        input_size=input_size,
        output_size=output_size,
        hidden_size=cfg.model.hidden_size,
    ).to(device)

    model = train_bc_model(model, data_loader, cfg, device=device)

    torch.save(model.actor.state_dict(), cfg.train.save_path)
    print(f"Model saved to {cfg.train.save_path}")

    final_success_rate = evaluate_success_rate(
        model, cfg.env.name, cfg.env.max_episode_steps, num_episodes=20
    )
    print(f"Final success rate: {final_success_rate:.3f}")


if __name__ == "__main__":
    main()
