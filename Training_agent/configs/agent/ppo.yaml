# configs/agent/ppo.yaml
# Hydra configuration for PPOAgent training by environment-step count.

env:
  # Gymnasium environment to train on
  name: MiniGrid-UnlockPickup-v0
  max_episode_steps: 6400

# Global random seed
seed: 0

# Whether to use CUDA for training
cuda: true

# Capture video of the agent's performance
capture_video: true

agent:
  # Learning rates (same for actor and critic)
  lr_actor: 2.5e-4    # actor step-size
  lr_critic: 2.5e-4    # critic step-size

  # Discount and GAE
  gamma: 0.99
  gae_lambda: 0.95

  # PPO-specific
  clip_eps: 0.2      # clipping epsilon for surrogate objective
  epochs: 4          # number of epochs to optimize the policy per update

  # Regularization coefficients
  ent_coef: 0.01     # entropy bonus weight
  vf_coef: 0.5       # value-loss weight

  # the maximum norm for the gradient clipping
  max_grad_norm: 0.5

  # the target KL divergence threshold for early stopping
  target_kl: 0.01

  # Network size
  hidden_size: 64
train:
  # Number of parallel environments
  num_envs: 4

  # Total environment interactions to run
  total_steps: 1500000
  # Number of steps to collect per environment before updating
  num_steps_env: 1600
  
  # number of mini-batches per update
  num_mini_batches: 4

  # Evaluation settings
  eval_interval: 10000  # evaluate every N steps
  eval_episodes: 10     # episodes per evaluation
